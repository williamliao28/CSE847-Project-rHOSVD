\documentclass[final]{elsarticle}
\usepackage{natbib}
\usepackage{bibentry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
% Packages for table
\usepackage{multirow}
\usepackage{longtable}
\usepackage{makecell}
\usepackage{array}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{ulem}
\usepackage{etoolbox}
\usepackage{tikz}
\usepackage{geometry}
\usepackage[mathscr]{eucal}

%\geometry{a4paper,left=1cm,right=1cm,top=1cm,bottom=1.5cm}

\usepackage{ulem}
\usepackage{color}
\newcommand{\rd}{\textcolor{red}}
\newcommand{\gr}{\textcolor{Green}}
\newcommand{\og}{\textcolor{Orange}}
\newcommand{\ce}{\textcolor{Cerulean}}
\newcommand{\tb}{\textcolor{blue}}

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}{Proposition}
\newdefinition{rmk}{Remark}
\newdefinition{defn}{Definition}
\newproof{pf}{Proof}
\newproof{pot}{Proof of Theorem \ref{thm2}}

\hypersetup{
colorlinks=true,
allcolors=.
}

\title{\textbf{\large{CSE 847 (Spring 2022): Machine Learning --- Project \\ Paper Study Summary}}}
\author{Wei-Chien Liao (\href{mailto:liaowei2@msu.edu}{liaowei2@msu.edu})}
\date{}

\begin{document}
%\maketitle
\begin{frontmatter}
\begin{abstract}
    This paper is a summary of basic concepts of tensors, Tucker decomposition and higher order singular value decomposition (HOSVD), and variants
    of randomized algorithms for computing these decompositions.
\end{abstract}
\begin{keyword}
    higher order singular value decomposition (HOSVD)\sep Tucker decomposition\sep randomized HOSVD
\end{keyword}
\end{frontmatter}
\section{Notation and Preliminaries}
\noindent The contents in this section are mainly based on \cite{Kolda2009}.
\begin{defn}
    The \textbf{order} of a tensor is the number of dimensions, also called \textbf{ways} or \textbf{modes}.
\end{defn}
In this paper,
\begin{itemize}
    \item \textbf{vectors} (tensors of order $1$) are denoeted by boldface lowercase letters, e.g. $\mathbf{a}$.
    \item \textbf{matrices} (tensors of order $2$) are denoeted by boldface capital letters, e.g. $\mathbf{A}$.
    \item \textbf{tensors} (order $\geq 3$) are denoeted by boldface Euler script letters, e.g. $\boldsymbol{\mathscr{X}}$.
    \item the $i$-th entry of a vector $\mathbf{a}$ is denoeted by $a_i$.
    \item the $(i,j)$-th element of a matrix $\mathbf{A}$ is denoeted by $A_{ij}$.
    \item the $(i,j,k)$-th element of a third-order tensor $\boldsymbol{\mathscr{X}}$ is denoeted by $x_{ijk}$.
    \item a colon ``$:$" is used to indicate all elements of a mode. e.g. for a matrix $\mathbf{A}$,
    \begin{itemize}
        \item $\mathbf{a}_{i:} = i$-th row of $\mathbf{A}$.
        \item $\mathbf{a}_{:j} = j$-th column of $\mathbf{A}$.
    \end{itemize}
\end{itemize}
\begin{defn}
    A \textbf{fiber} is defined by fixing every index but one.
\end{defn}
For a third-order tensor $\boldsymbol{\mathscr{X}}$,
\begin{itemize}
    \item $\mathbf{x}_{:jk}=$ \textbf{column fibers} or \textbf{mode-1 fibers}  of $\boldsymbol{\mathscr{X}}$.
    \item $\mathbf{x}_{i:k}=$ \textbf{row fibers} or \textbf{mode-2 fibers}  of $\boldsymbol{\mathscr{X}}$.
    \item $\mathbf{x}_{ij:}=$ \textbf{tube fibers} or \textbf{mode-3 fibers}  of $\boldsymbol{\mathscr{X}}$.
\end{itemize}
\begin{defn}
    \textbf{Slices} are two-dimensional sections of a tensor defined by fixing all but two indices.
\end{defn}
For a third-order tensor $\boldsymbol{\mathscr{X}}$,
\begin{itemize}
    \item $\mathbf{X}_{i::}=$ \textbf{horizontal slices} of $\boldsymbol{\mathscr{X}}$.
    \item $\mathbf{X}_{:j:}=$ \textbf{lateral slices} of $\boldsymbol{\mathscr{X}}$.
    \item $\mathbf{X}_{::k}=$ \textbf{frontal slices} of $\boldsymbol{\mathscr{X}}$.
\end{itemize}
\begin{defn}[Norm of a Tensor]
    The \textbf{norm} of a tensor $\boldsymbol{\mathscr{X}}\in\mathbb{R}^{I_1\times I_2\times\cdots\times I_N}$,
    denoeted by $||\boldsymbol{\mathscr{X}}||$, is defined as
    \begin{equation}
        ||\boldsymbol{\mathscr{X}}||=\sqrt{\sum_{i_1=1}^{I_1}\sum_{i_2=1}^{I_2}\cdots\sum_{i_N=1}^{I_N}x_{i_1i_2\cdots i_N}^2}.
    \end{equation}
\end{defn}
\begin{defn}[Inner Product of Tensors]
    The \textbf{inner product} of two same-sized tensors\\
    $\boldsymbol{\mathscr{X}},\boldsymbol{\mathscr{Y}}\in\mathbb{R}^{I_1\times I_2\times\cdots\times I_N}$,
    denoeted by $\langle\boldsymbol{\mathscr{X}},\boldsymbol{\mathscr{Y}}\rangle$, is defined as
    \begin{equation}
        \langle\boldsymbol{\mathscr{X}},\boldsymbol{\mathscr{Y}}\rangle=\sqrt{\sum_{i_1=1}^{I_1}\sum_{i_2=1}^{I_2}\cdots\sum_{i_N=1}^{I_N}x_{i_1i_2\cdots i_N}y_{i_1i_2\cdots i_N}}.
    \end{equation}
\end{defn}
Thus, by the definition of norm and inner product,
$\langle\boldsymbol{\mathscr{X}},\boldsymbol{\mathscr{X}}\rangle=||\boldsymbol{\mathscr{X}}||^2$.
\begin{defn}[Rank-one Tensors]
    A $N$-way tensor $\boldsymbol{\mathscr{X}}\in\mathbb{R}^{I_1\times I_2\times\cdots\times I_N}$ is \textbf{rank one} if it can
    be written as the outer product of $N$ vectors,
    \begin{equation}
        \boldsymbol{\mathscr{X}} = \mathbf{a}^{(1)}\circ\mathbf{a}^{(2)}\circ\cdots\circ\mathbf{a}^{(N)},
    \end{equation}
    for some vectors $\mathbf{a}^{(1)},\mathbf{a}^{(2)},\cdots,\mathbf{a}^{(N)}$ and ``$\circ$'' denotes the vector outer product.
\end{defn}
\begin{defn}
    A tensor is called \textbf{cubical} if every mode is the same size. A cubical tensor is called \textbf{supersymmetric}
    (some literatures call this ``symmetric'') if its elements remain constant under any permutation of the indices.
\end{defn}
For a $3$-way tensor $\boldsymbol{\mathscr{X}}\in\mathbb{R}^{I\times I\times I}$, it is supersymmetric if
\[
    x_{ijk} = x_{ikj} = x_{jik} = x_{jki} = x_{kij} = x_{kji} \quad \forall i,j,k=1,\cdots I.
\]
\begin{defn}[Diagnoal Tensor]
    A tensor $\boldsymbol{\mathscr{X}}\in\mathbb{R}^{I_1\times I_2\times\cdots\times I_N}$ is \textbf{diagonal}
    if $x_{i_1i_2\cdots i_N}\neq 0$ only if $i_1=i_2=\cdots=i_N$. 
\end{defn}
\begin{defn}[Matricization]
    The process of reordering the elements of an $N$-way array into a matrix is called \textbf{matricization}
This is also called \textbf{unfolding} or \textbf{flattening}.
\end{defn}
The mode-$n$ matricization of a tensor $\boldsymbol{\mathscr{X}}\in\mathbb{R}^{I_1\times I_2\times\cdots\times I_N}$
is denoeted by $\mathbf{X}_{(n)}$ and arranges the mode-$n$ fibers to be the columns of the resulting matrix.
\begin{rmk}
    It is also possible to vectorize a tensor. This process is called vectorization.
\end{rmk}
\subsection{Tensor Mulitiplication}
\noindent The \textbf{$n$-mode prodcut} of a tensor $\boldsymbol{\mathscr{X}}\in\mathbb{R}^{I_1\times I_2\times\cdots\times I_N}$
with a matrix $\mathbf{U}\in\mathbb{R}^{J\times I_n}$ is defined as
\begin{subequations}
    \begin{align}
        & \text{eletmenwise: } \quad \left(\boldsymbol{\mathscr{X}}\times_n\mathbf{U}\right)_{i_1\cdots i_{n-1}ji_{n+1}\cdots i_N}
        =\sum_{i_n=1}^{I_n}x_{i_1i_2\cdots i_N}u_{ji_n} \\
        & \text{unfold tensors: } \quad \boldsymbol{\mathscr{Y}} = \boldsymbol{\mathscr{X}}\times_n\mathbf{U}\Leftrightarrow
        \mathbf{Y}_{(n)} = \mathbf{U}\mathbf{X}_{(n)}
    \end{align}
\end{subequations}
The result is a tensor of size $I_1\times\cdots\times I_{n-1}\times J\times I_{n+1}\times\cdots\times I_N$. Each mode-$n$ fiber is multiplied by the matrix $\mathbf{U}$.
The following are some properties of the $n$-mode prodcut:
\begin{enumerate}
    \item[(1)] For distinct modes
    in a series of multiplications, the order of the multiplication is irrelevant.
    \begin{equation}
        \boldsymbol{\mathscr{X}}\times_m\mathbf{A}\times_n\mathbf{B} = \boldsymbol{\mathscr{X}}\times_n\mathbf{B}\times_m\mathbf{A},\quad \text{for } n\neq m.
    \end{equation}
    \item[(2)] If the modes are the same
    \begin{equation}
        \boldsymbol{\mathscr{X}}\times_n\mathbf{A}\times_n\mathbf{B}=\boldsymbol{\mathscr{X}}\times_n(\mathbf{B}\mathbf{A}).
    \end{equation} 
\end{enumerate}
\subsection{Some Matrix Products}
\begin{itemize}
    \item \underline{\textbf{Kronecker Product}}\\[0.3cm]
    The \textbf{Kronecker product} of two matrices $\mathbf{A}\in\mathbb{R}^{I\times J}$ and $\mathbf{B}\in\mathbb{R}^{K\times L}$,
    denoeted by $\mathbf{A}\otimes\mathbf{B}$, is defined as
    \begin{equation}
        \mathbf{A}\otimes\mathbf{B}=\begin{bmatrix}
            a_{11}\mathbf{B} & a_{12}\mathbf{B} & \cdots & a_{1J}\mathbf{B} \\
            a_{21}\mathbf{B} & a_{22}\mathbf{B} & \cdots & a_{2J}\mathbf{B} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{I1}\mathbf{B} & a_{I2}\mathbf{B} & \cdots & a_{IJ}\mathbf{B}
        \end{bmatrix}
    \end{equation}
    \item \underline{\textbf{Khatri-Rao Product}}\\[0.3cm]
    The \textbf{Khatri-Rao product} of two matrices $\mathbf{A}\in\mathbb{R}^{I\times K}$ and $\mathbf{B}\in\mathbb{R}^{J\times K}$,
    denoeted by $\mathbf{A}\odot\mathbf{B}$, is defined as
    \begin{equation}
        \mathbf{A}\odot\mathbf{B}=\begin{bmatrix}
            \mathbf{a}_{1}\otimes\mathbf{b}_1 & \mathbf{a}_{2}\otimes\mathbf{b}_2 & \cdots & \mathbf{a}_{K}\otimes\mathbf{b}_K
        \end{bmatrix}
    \end{equation}
    \item \underline{\textbf{Hadamard Product}}\\[0.3cm]
    The \textbf{Hadamard product} of two matrices $\mathbf{A}, \mathbf{B}\in\mathbb{R}^{I\times J}$,
    denoeted by $\mathbf{A}*\mathbf{B}$, is defined as
    \begin{equation}
        \mathbf{A}*\mathbf{B}=\begin{bmatrix}
            a_{11}b_{11} & a_{12}b_{12} & \cdots & a_{1J}b_{1J} \\
            a_{21}b_{21} & a_{22}b_{22} & \cdots & a_{2J}b_{2J} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{I1}b_{I1} & a_{I2}b_{I2} & \cdots & a_{IJ}b_{IJ}
        \end{bmatrix}
    \end{equation}
\end{itemize}
Some properties of these matrix prodcuts:
\begin{subequations}
    \begin{align}
        (\mathbf{A}\otimes\mathbf{B})(\mathbf{C}\otimes\mathbf{D}) &= \mathbf{A}\mathbf{C}\otimes\mathbf{B}\mathbf{D} \\
        (\mathbf{A}\otimes\mathbf{B})^{\dagger} &= \mathbf{A}^{\dagger}\otimes\mathbf{B}^{\dagger} \\
        \mathbf{A}\odot\mathbf{B}\odot\mathbf{C} &= (\mathbf{A}\odot\mathbf{B})\odot\mathbf{C} = \mathbf{A}\odot(\mathbf{B}\otimes\mathbf{C}) \\
        (\mathbf{A}\odot\mathbf{B})^T(\mathbf{A}\odot\mathbf{B}) &= (\mathbf{A}^T\mathbf{A})*(\mathbf{B}^T\mathbf{B}) \\
        (\mathbf{A}\odot\mathbf{B})^{\dagger} &= (\mathbf{A}^T\mathbf{A})*(\mathbf{B}^T\mathbf{B})^{\dagger}(\mathbf{A}\odot\mathbf{B})^T
    \end{align}
\end{subequations}
\section{Tucker Decomposition and HOSVD}
\section{Randomized Algorithms}
\noindent This section summarizes some randomized algorithm for computing HOSVD discussed in \cite{9350569}.
\bibliographystyle{plain}
\bibliography{msu-cse847-project}
\end{document}